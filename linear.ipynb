{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vkqdqU79vA9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Simple linear regression models the relationship between two variables by fitting a straight line through data points. It helps predict the dependent variable (Y) based on the independent variable (X)"
      ],
      "metadata": {
        "id": "aLqeYp8d07vG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Linearity: The relationship between X and Y must be linear (e.g., temperature vs. mercury expansion in a thermometer).\n",
        "\n",
        "Homoscedasticity: Residuals (errors) have constant variance across X values.\n",
        "\n",
        "Independence: Observations are not influenced by each other (e.g., data collected from unrelated individuals).\n",
        "\n",
        "Normality: Residuals follow a normal distribution"
      ],
      "metadata": {
        "id": "zracmxodwWkA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Coefficients in\n",
        "Y\n",
        "=\n",
        "m\n",
        "X\n",
        "+\n",
        "c\n",
        "Y=mX+c\n",
        "\n",
        "Slope (m): Represents the change in Y for a 1-unit increase in X.\n",
        "\n",
        "Example: If\n",
        "m\n",
        "=\n",
        "2\n",
        "m=2, Y increases by 2 units for every 1-unit rise in X.\n",
        "\n",
        "Intercept (c): The value of Y when\n",
        "X\n",
        "=\n",
        "0\n",
        "X=0.\n",
        "\n",
        "Example: If\n",
        "c\n",
        "=\n",
        "5\n",
        "c=5, Y is 5 when X is 0"
      ],
      "metadata": {
        "id": "L0AWOw3qw_kI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.The formula for slope is:\n",
        "m\n",
        "=\n",
        "r\n",
        "⋅\n",
        "S\n",
        "y\n",
        "S\n",
        "x\n",
        "m=r⋅\n",
        "S\n",
        "x\n",
        "\n",
        "S\n",
        "y\n",
        "\n",
        "\n",
        "\n",
        "r\n",
        "r: Correlation between X and Y.\n",
        "\n",
        "S\n",
        "y\n",
        "S\n",
        "y\n",
        " : Standard deviation of Y.\n",
        "\n",
        "S\n",
        "x\n",
        "S\n",
        "x\n",
        " : Standard deviation of X."
      ],
      "metadata": {
        "id": "PatnLk7GxOMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The slope\n",
        "m\n",
        "m (or\n",
        "b\n",
        "b) in simple linear regression is calculated using the formula:\n",
        "\n",
        "m\n",
        "=\n",
        "r\n",
        "×\n",
        "S\n",
        "y\n",
        "S\n",
        "x\n",
        "m=r×\n",
        "S\n",
        "x\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n7EEjJ7KxfnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.The least squares method is used to find the line of best fit for a set of data points. It does this by minimizing the sum of the squares of the residuals (the vertical distances between the observed data points and the predicted values on the regression line). This ensures the regression line best represents the overall trend in the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "oxomyQGcySid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LM14OILOxWDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.The coefficient of determination (\n",
        "R\n",
        "2\n",
        "R\n",
        "2\n",
        " ) measures the proportion of variance in the dependent variable (Y) that is explained by the independent variable (X) in the model. An\n",
        "R\n",
        "2\n",
        "R\n",
        "2\n",
        "  value of 0.8 means that 80% of the variability in Y is explained by X. Higher\n",
        "R\n",
        "2\n",
        "R\n",
        "2\n",
        "  values indicate a better fit of the model to the data."
      ],
      "metadata": {
        "id": "mZJUt6m5ylwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Multiple linear regression is an extension of simple linear regression that models the relationship between a dependent variable (Y) and two or more independent variables (\n",
        "X\n",
        "1\n",
        ",\n",
        "X\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "X\n",
        "n\n",
        "X\n",
        "1\n",
        " ,X\n",
        "2\n",
        " ,...,X\n",
        "n\n",
        " ). The general equation is:\n",
        "\n",
        "Y\n",
        "=\n",
        "b\n",
        "0\n",
        "+\n",
        "b\n",
        "1\n",
        "X\n",
        "1\n",
        "+\n",
        "b\n",
        "2\n",
        "X\n",
        "2\n",
        "+\n",
        ".\n",
        ".\n",
        ".\n",
        "+\n",
        "b\n",
        "n\n",
        "X\n",
        "n\n",
        "Y=b\n",
        "0\n",
        " +b\n",
        "1\n",
        " X\n",
        "1\n",
        " +b\n",
        "2\n",
        " X\n",
        "2\n",
        " +...+b\n",
        "n\n",
        " X\n",
        "n"
      ],
      "metadata": {
        "id": "qyxJgOqTy0Ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9 .Simple Linear Regression\t=\n",
        "Models relationship between Y and one X\n",
        "Equation:\n",
        "Y\n",
        "=\n",
        "m\n",
        "X\n",
        "+\n",
        "c\n",
        "Y=mX+c\n",
        "\n",
        "Multiple Linear Regression =\n",
        "Models relationship between Y and two or more Xs\n",
        "Equation:\n",
        "Y\n",
        "=\n",
        "b\n",
        "0\n",
        "+\n",
        "b\n",
        "1\n",
        "X\n",
        "1\n",
        "+\n",
        "b\n",
        "2\n",
        "X\n",
        "2\n",
        "+\n",
        ".\n",
        ".\n",
        ".\n",
        "Y=b\n",
        "0\n",
        " +b\n",
        "1\n",
        " X\n",
        "1\n",
        " +b\n",
        "2\n",
        " X\n",
        "2\n",
        " +..."
      ],
      "metadata": {
        "id": "TSmqfzCyzcYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.Linearity: The relationship between each independent variable and the dependent variable is linear.\n",
        "\n",
        "Independence: Observations are independent of each other.\n",
        "\n",
        "Homoscedasticity: The variance of residuals is constant across all levels of the independent variables.\n",
        "\n",
        "No multicollinearity: Independent variables are not highly correlated with each other.\n",
        "\n",
        "Normality: Residuals are normally distributed."
      ],
      "metadata": {
        "id": "exuC3qUFzuXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.Heteroscedasticity occurs when the variance of the residuals is not constant across all levels of the independent variables. This violates the homoscedasticity assumption and can lead to:\n",
        "\n",
        "Inefficient estimates of regression coefficients (though they remain unbiased)\n",
        "\n",
        "Invalid standard errors, which can make hypothesis tests and confidence intervals unreliable\n",
        "\n",
        "Potentially misleading statistical inferences"
      ],
      "metadata": {
        "id": "E6stB4T0021p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.Remove or Combine Predictors: Drop one of the highly correlated variables or combine them into a single predictor.\n",
        "\n",
        "Principal Component Analysis (PCA): Transform correlated variables into a set of uncorrelated components.\n",
        "\n",
        "Regularization Techniques: Use methods like Ridge or Lasso regression, which penalize large coefficients and can reduce the impact of multicollinearity."
      ],
      "metadata": {
        "id": "RjdivN8h1bu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.Dummy Coding: Convert each category into a separate binary (0/1) variable, with one category omitted as the reference group.\n",
        "\n",
        "Simple Coding: Compare each level to a specific reference level, often the lowest or a meaningful baseline.\n",
        "\n",
        "Other Coding Schemes: Depending on the analysis, alternatives like effect coding or ordinal coding can be used, but dummy coding is the most common"
      ],
      "metadata": {
        "id": "Ezm92tEX1iMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.Interaction terms allow the effect of one predictor on the outcome to depend on the value of another predictor. They are constructed by multiplying two (or more) predictors together and including the product as an additional variable in the model. This enables the model to capture non-additive relationships and test whether the influence of one variable changes at different levels of another"
      ],
      "metadata": {
        "id": "lXk7dxfW1zP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.Simple Linear Regression: The intercept represents the expected value of the dependent variable when the independent variable is zero.\n",
        "\n",
        "Multiple Linear Regression: The intercept is the expected value of the dependent variable when all independent variables are set to zero. In practice, this scenario may not be meaningful if zero is not a plausible value for all predictors, so the interpretation can be less intuitive in multiple regression."
      ],
      "metadata": {
        "id": "8ERk5AnE2HMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.The slope quantifies the expected change in the dependent variable for a one-unit increase in the independent variable, holding all other variables constant. It directly affects predictions: larger slopes indicate a stronger relationship between the predictor and the outcome."
      ],
      "metadata": {
        "id": "VdD1u2uQ2Q3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.The intercept serves as a baseline or starting value for the dependent variable when all predictors are zero. It contextualizes the regression equation, anchoring the predicted values and allowing for meaningful interpretation of the slopes relative to this baseline."
      ],
      "metadata": {
        "id": "PySIog5h2fvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.Does Not Indicate Causality: A high R² does not mean that predictors cause changes in the outcome.\n",
        "\n",
        "Insensitive to Overfitting: R² can be artificially inflated by adding more variables, even if they are not meaningful.\n",
        "\n",
        "No Insight on Predictive Accuracy: R² reflects fit to the training data, not necessarily predictive performance on new data.\n",
        "\n",
        "Ignores Model Assumptions: R² does not reveal violations of regression assumptions such as homoscedasticity or multicollinearity.\n",
        "\n",
        "Does Not Reflect Variable Importance: It does not show which predictors are most influential."
      ],
      "metadata": {
        "id": "mIYFYGc02qFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.A large standard error for a regression coefficient indicates high variability in the estimate, meaning there is less confidence that the estimated coefficient is close to the true population value. This can result from factors such as small sample size, high variability in the data, or multicollinearity among predictors. When the standard error is large relative to the coefficient, the effect of that variable is less precisely estimated, making statistical inference less reliable"
      ],
      "metadata": {
        "id": "LEufe6Mj2y6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.Heteroscedasticity is detected by visually inspecting residual plots, particularly the plot of residuals versus fitted values. If the residuals display a funnel or cone shape—where the spread of residuals increases or decreases with fitted values—this suggests heteroscedasticity. In contrast, homoscedasticity is indicated by a random scatter of residuals with constant variance. Addressing heteroscedasticity is important because it can lead to inefficient parameter estimates and unreliable hypothesis tests, affecting the validity of statistical inference"
      ],
      "metadata": {
        "id": "Qjiko6Yq3BUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.If a multiple linear regression model has a high R² but a low adjusted R², it suggests that the model includes predictors that do not meaningfully contribute to explaining the variance in the dependent variable. Adjusted R² penalizes the addition of unnecessary predictors, so a large gap between R² and adjusted R² indicates possible overfitting or inclusion of irrelevant variables"
      ],
      "metadata": {
        "id": "DpLrFlPr3MoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.Scaling variables (standardizing or normalizing) ensures that all predictors contribute equally to the analysis, especially when variables are measured on different scales. Without scaling, variables with larger ranges can disproportionately influence the model, and the interpretation of coefficients becomes challenging. Scaling also improves the performance of optimization algorithms used in model fitting."
      ],
      "metadata": {
        "id": "nEIbuZPO3Q-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.Polynomial regression is a form of regression analysis where the relationship between the independent variable(s) and the dependent variable is modeled as an nth-degree polynomial."
      ],
      "metadata": {
        "id": "tMv4YFd63YS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.Linear Regression =\n",
        "Models a straight-line relationship\n",
        "Equation:\n",
        "Y\n",
        "=\n",
        "b\n",
        "0\n",
        "+\n",
        "b\n",
        "1\n",
        "X\n",
        "Y=b\n",
        "0\n",
        " +b\n",
        "1\n",
        " X\n",
        "\n",
        "Polynomial Regression =\n",
        "Models a curved (non-linear) relationship\n",
        "Equation:\n",
        "Y\n",
        "=\n",
        "b\n",
        "0\n",
        "+\n",
        "b\n",
        "1\n",
        "X\n",
        "+\n",
        "b\n",
        "2\n",
        "X\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "b\n",
        "n\n",
        "X\n",
        "n\n",
        "Y=b\n",
        "0\n",
        " +b\n",
        "1\n",
        " X+b\n",
        "2\n",
        " X\n",
        "2\n",
        " +⋯+b\n",
        "n\n",
        " X\n",
        "n"
      ],
      "metadata": {
        "id": "7ewF3QEU3eZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.Polynomial regression is used when data shows a non-linear relationship that cannot be captured by a straight line, such as U-shaped or S-shaped trends.Y=b\n",
        "0\n",
        " +b\n",
        "1\n",
        " X+b\n",
        "2\n",
        " X\n",
        "2\n",
        " +⋯+b\n",
        "n\n",
        " X\n",
        "n"
      ],
      "metadata": {
        "id": "GgoSalU03yyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26.Y=b 0 +b 1 X+b 2 X 2 +⋯+b n X n"
      ],
      "metadata": {
        "id": "fdvkUywD36fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27.Yes, polynomial regression can be extended to multiple variables by including polynomial and interaction terms for each predictor"
      ],
      "metadata": {
        "id": "b-pP2KBz4NGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28.Prone to overfitting, especially with high-degree polynomials\n",
        "\n",
        "Sensitive to outliers\n",
        "\n",
        "Can become computationally expensive with many variables or high degrees\n",
        "\n",
        "Interpretation of coefficients becomes complex as the degree increases"
      ],
      "metadata": {
        "id": "DCbQZqw24b2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29.Cross-validation (e.g., k-fold)\n",
        "\n",
        "Comparing metrics such as adjusted R², AIC, or BIC\n",
        "\n",
        "Residual analysis to check for patterns\n",
        "\n",
        "Visualization of fitted curve versus data"
      ],
      "metadata": {
        "id": "bzYW8hkE4lpm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30.Visualization helps assess how well the polynomial curve fits the data, detect overfitting or underfitting, and communicate model results effectively."
      ],
      "metadata": {
        "id": "DKKMfbRd4vKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#31.\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Example: 2nd-degree polynomial regression\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "ubT_2-c55Azp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8smzdJgk406S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ah0KAj9K47Ym"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}